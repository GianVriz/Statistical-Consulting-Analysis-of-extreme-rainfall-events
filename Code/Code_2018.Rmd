---
title: "Satellite data 2018"
author: "Vriz Gian Luca"
date: "2023-04-26"
output: html_document
---

# Satellite data

## Wet days

Firstly, we load all libraries,

```{r setup, include=FALSE}
library(readr) 
library(data.table)
library(ggplot2)
library(R.matlab)
library(ggspatial)
library(EnvStats)
library(plyr)
library(dplyr)
library(tseries)
library(xgboost)
library(keras)
library(mgcv)
library(Metrics)
library(raster)
library(sp)
library(plyr)
```

Then, we import the dataset, which is a three dimensional object with latitude, longitude and time series of wet days (value higher than 1mm).

```{r}
data_2018_wet <- readMat('Satellite data/Wet/Satellite_wet_2018.mat')
```

Here, we obtain the number of wet days for each location.

```{r}
#Count of wet days take time
Count <- c()
for (i in 1: length(data_2018_wet[['Obj']][[3]])){
if(sum(array(unlist(data_2018_wet[['Obj']][[3]][[i]]))) == 0){Count <- append(Count, 0)}

else{
a <- array(unlist(data_2018_wet[['Obj']][[3]][[i]]))
Count <- append(Count, length(a))
   }}
Count
```

Satellite data.

```{r}
satellite_2018 <- as.data.frame(Count, colnames('Count'))
satellite_2018$Latitude <- as.numeric(data_2018_wet[["Obj"]][[1]])
satellite_2018$Longitude <- as.numeric(data_2018_wet[["Obj"]][[2]])
View(satellite_2018)
```

Plot wet days.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
world <- map_data("world", maptype = 'terrain-background',)
world <- world[world$lat <= 60 & world$lat > -60 & world$long <= 180 & world$lat > -180,]

map_1 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = satellite_2018,
    aes(Longitude, Latitude, color= Count), alpha = 0.008
  ) +
 labs(title = 'Satellite', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn( colours = rainbow(3), name = "Wet days")
map_1
```

## Weibull distribution using MLE

As suggested in Marra, F. et al. (2019), we  a dynamic threshold, that is ordinary events with a value higher than the 75th quantile.

```{r}
data_2018_75 <- readMat('Satellite data/Quantile/Satellite_2018_75.mat')
```

We first estimates the parameters of the Weibull model considering MLE.

```{r,warning=FALSE}
#Estimation of scale and shape parameter: take time
Scale <- c()
Shape <- c()
for (i in 1:length(data_2018_75[['Obj']][[3]])){

a <- array(unlist(data_2018_75[['Obj']][[3]][[i]]))
if(length(unique(a)) <= 1){Scale <- append(Scale, 0)
   Shape <- append(Shape, 0)}

else{
a <- array(unlist(data_2018_75[['Obj']][[3]][[i]]))
est <- eweibull(as.numeric(a), method = "mle")
Scale <- append(Scale, est$parameters[2])
Shape <- append(Shape, est$parameters[1])
   }}
```

Satellite data.

```{r}
satellite_2018 <- as.data.frame(Shape, colnames('Shape'))
satellite_2018$Scale <- Scale
satellite_2018$Latitude <- array(unlist(data_2018_75[["Obj"]][[2]]))
satellite_2018$Longitude <- array(unlist(data_2018_75[["Obj"]][[1]]))
View(satellite_2018)
```

Plot scale parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_2 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = satellite_2018,
    aes(Longitude, Latitude, color= Scale), size = 1, alpha = 0.008
  ) +
 labs(title = 'Satellite', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,60),colours = rainbow(3), name = "Scale parameter")
map_2
```

Histogram scale parameter.

```{r}
quantile(Scale, 0.99) #quantile 99
hist(Scale, breaks=50, col="red", xlab="Scale parameter", xlim=c(0,100),
   main="Histogram")
```

Plot shape parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_3 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = satellite_2018,
    aes(Longitude, Latitude, color= Shape), size = 1, alpha = 0.008
  ) +
 labs(title = 'Satellite', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,4),colours = rainbow(3), name = "Shape parameter")
map_3
```

Histogram shape parameter.

```{r}
quantile(Shape, 0.99) #quantile 99
hist(Shape, breaks = 10000, col = "red", xlab = "Shape parameter", xlim = c(0,10),
   main="Histogram")
```

# Station data

In the following, we will perform the same procedure considering station data.

## Wet days

Firstly, we have to obtain the number of wet days for each station.

```{r}
data_station <- readMat('Station data/Rain_data_2018_mat_75_alt.mat')
```

Station data.

```{r}
#The number of wet days was performed in Python: faster
Count_2 <- t(data_station[["Obj"]][[5]])
station_2018 <- as.data.frame(Count_2)
station_2018$Latitude <- as.numeric(data_station[["Obj"]][[1]])
station_2018$Longitude <- as.numeric(unlist(data_station[["Obj"]][[2]]))
colnames(station_2018)[1] = "Count"
station_2018 <- station_2018[station_2018$Latitude <= 60 & station_2018$Latitude > -60 & station_2018$Longitude <= 180 & station_2018$Longitude > -180,]
View(station_2018)
```

Plot wet days.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_4 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = station_2018,
    aes(Longitude, Latitude, color= Count), size = 1
  )+
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,365),colours = rainbow(3), name = "Wet days")
map_4
```

## Weibull distribution using MLE

Then, we will estimate the parameters of the Weibull distribution using MLE.

```{r,warning=FALSE}
#Estimation of scale and shape: take time.
Scale_2 <- c()
Shape_2 <- c()
for (i in 1:length(data_station[['Obj']][[4]])){

a <- array(unlist(data_station[['Obj']][[4]][[i]]))
if(length(unique(a)) <= 1){Scale_2 <- append(Scale_2, 0)
   Shape_2 <- append(Shape_2, 0)}

else{
a <- array(unlist((data_station[['Obj']][[4]][[i]])))
est_2 <- eweibull(as.numeric(a), method = "mle")
Scale_2 <- append(Scale_2, est_2$parameters[2])
Shape_2 <- append(Shape_2, est_2$parameters[1])
   }}
```

Station data.

```{r}
station_2018 <- as.data.frame(Shape_2, colnames('Shape'))
station_2018$Scale <- Scale_2
station_2018$Latitude <- as.numeric(data_station[["Obj"]][[1]])
station_2018$Longitude <- as.numeric(data_station[["Obj"]][[2]])
colnames(station_2018)[1] = "Shape"
station_2018 <- station_2018[station_2018$Latitude <= 60 & station_2018$Latitude > -60 & station_2018$Longitude <= 180 & station_2018$Longitude > -180,]
```

Plot scale parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_5 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = station_2018,
    aes(Longitude, Latitude, color= Scale), size = 1
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,60),colours = rainbow(3), name = "Scale parameter")
map_5
```

Plot shape parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_6 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = station_2018,
    aes(Longitude, Latitude, color= Shape), size = 1
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,4),colours = rainbow(3), name = "Shape parameter")
map_6
```

# Weibull distribution using LS

In the following we estimate the parameters using also the least squares regression in Weibull-transformed coordinates approach (Marra, F. et al., 2019).

## Satellite data

Least squares regression in Weibull-transformed coordinates for satellite data.

```{r warning = FALSE}
#Estimation of shape and scale parameter: take time
Scale_3 <- c()
Shape_3 <- c()
for (i in 1:length(data_2018_75[['Obj']][[3]])){
a <- array(unlist(data_2018_75[['Obj']][[3]][[i]]))

if(length(unique(a)) <= 1){Scale_3 <- append(Scale_3, 0)
   Shape_3<-append(Shape_3,0)}

else{
Y <- log(sort(a))
F <- (c(1:length(a))/(length(a)+1))
X <- log(-log(1-F))
reg <- lm(Y~X) 

shape <- 1/reg[["coefficients"]][["X"]]
scale <- exp(reg[["coefficients"]][["(Intercept)"]])

Shape_3 <- append(Shape_3, shape)
Scale_3 <- append(Scale_3, scale)
   }}
Scale_3
```

Satellite data.

```{r}
satellite_2018 <- as.data.frame(Shape_3, colnames('Shape'))
satellite_2018$Scale <- Scale_3
satellite_2018$Latitude <- array(unlist(data_2018_75[["Obj"]][[2]]))
satellite_2018$Longitude <- array(unlist(data_2018_75[["Obj"]][[1]]))
colnames(satellite_2018)[1] = "Shape"
```

Plot scale parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_7 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = satellite_2018,
    aes(Longitude, Latitude, color= Scale), size = 1, alpha = 0.008
  ) +
 labs(title = 'Satellite', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,60), colours = rainbow(3), name = "Scale Parameter")
map_7
```

Histogram scale parameter.

```{r}
quantile(Scale_3, 0.99) #quantile 99
hist(Scale_3, breaks=100, col="red", xlab="Scale parameter", xlim=c(0,100),
   main="Histogram")
```

Plot shape parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_8 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = satellite_2018,
    aes(Longitude, Latitude, color= Shape), size = 1, alpha = 0.008
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits=c(0,5),colours = rainbow(3), name = "Shape parameter")
map_8
```

Histogram shape parameter.

```{r}
quantile(Shape_3, 0.99) #99 quantile
hist(Shape_3, breaks=10000, col="red", xlab="Scale parameter", xlim=c(0,10),
   main="Histogram")
```

## Station data

Least squares regression in Weibull-transformed coordinates for station data.

```{r,warning=FALSE}
#Estimation of scale and shape parameter: take time
Scale_4 <- c()
Shape_4 <- c()
for (i in 1:length(data_station[['Obj']][[4]])){
a <- array(unlist(data_station[['Obj']][[4]][[i]]))

if(length(unique(a)) <= 1){Scale_4 <- append(Scale_4, 0)
   Shape_4<-append(Shape_4,0)}

else{
Y <- log(sort(a))
F <- (c(1:length(a))/(length(a)+1))
X <- log(-log(1-F))
reg <- lm(Y~X) 

shape <- 1/reg[["coefficients"]][["X"]]
scale <- exp(reg[["coefficients"]][["(Intercept)"]])

Shape_4 <- append(Shape_4, shape)
Scale_4 <- append(Scale_4, scale)
   }}
Scale_4
```

Station data.

```{r}
station_2018 <- as.data.frame(Shape_4, colnames('Shape'))
station_2018$Scale <- Scale_4
station_2018$Latitude <- as.numeric(data_station[["Obj"]][[1]])
station_2018$Longitude <- as.numeric(data_station[["Obj"]][[2]])
colnames(station_2018)[1] = "Shape"
station_2018$Count <- Count_2
station_2018 <- station_2018[station_2018$Latitude < 60 & station_2018$Latitude > -60 & station_2018$Longitude < 180 & station_2018$Longitude > -180,]
```

Plot scale parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_9 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = station_2018,
    aes(Longitude, Latitude, color= Scale), size = 1
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,60), colours = rainbow(3), name = "Scale parameter")
map_9
```

Plot shape parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_10 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = station_2018,
    aes(Longitude, Latitude, color= Shape), size = 1
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(0,4),colours = rainbow(3), name = "Shape parameter")
map_10
```

## Matching values for error correction

In the following we will merge data station and satellite data by coordinates. The coordinates in the dataset are in different format, then we have to approximate to merge all values.

```{r}
satellite_2018 <- as.data.frame(Shape_3, colnames('Shape'))
satellite_2018$Scale <- Scale_3
satellite_2018$Latitude <- array(unlist(data_2018_75[["Obj"]][[2]]))
satellite_2018$Longitude <- array(unlist(data_2018_75[["Obj"]][[1]]))
colnames(satellite_2018)[1] = "Shape"
satellite_2018$Count<-Count
#satellite_2018$Max<-as.numeric(Max)
#satellite_2018$Mean<-Mean


station_2018 <- as.data.frame(Shape_4, colnames('Shape'))
station_2018$Scale <- Scale_4
station_2018$Latitude <- as.numeric(data_station[["Obj"]][[1]])
station_2018$Longitude <- as.numeric(data_station[["Obj"]][[2]])
colnames(station_2018)[1] = "Shape"
station_2018$Altitude<-as.numeric(data_station[["Obj"]][[3]])
station_2018$Count<-Count_2
station_2018 <- station_2018[station_2018$Latitude < 60 & station_2018$Latitude > -60 & station_2018$Longitude < 180 & station_2018$Longitude > -180,]

satellite_2018$Cord <-apply(satellite_2018[,c("Latitude","Longitude")], 1, FUN=function(x) {paste(x[1],x[2],sep=", ") })

station_2018$Cord <-apply(station_2018[,c("Latitude","Longitude")], 1, FUN=function(x) {paste(round_any(x[1],0.250)-0.125,round_any(x[2],0.250)-0.125,sep=", ") })

sat_stat_2018 <- subset(satellite_2018, Cord %in% as.vector(station_2018$Cord))
sat_stat_2018 <- sat_stat_2018 %>% dplyr::select(1,2,5,6)

all<-merge(sat_stat_2018, station_2018, by=c("Cord","Cord"))
```

Now, we obtain errors as well as the MSE between satellite data and station data.

```{r}
all$err_shape <- all$Shape.y-all$Shape.x
all$err_scale <- all$Scale.y-all$Scale.x
all$err_wet <- all$Count.y-all$Count.x

mean(all$err_wet^2)
mean(all$err_scale^2)
mean(all$err_shape^2)
```

We both normality and stationary assumptions for the errors.

```{r}
shapiro.test(all$err_wet)
hist(all$err_wet, xlab='Station', ylab='Error', main='Histogram')
plot(density(all$err_wet),xlab='Station', ylab='Error', main='Density')
abline(v=quantile(all$err_wet, c(0.05,0.95)), lty=2, col ='red')

plot(all$err_wet, type='l', xlab='Station', ylab='Error wet days', main='Series')
abline(h=mean(all$err_wet), col='red', type='dashed')

adf.test(all$err_wet)
```

```{r}
shapiro.test(all$err_scale)
hist(all$err_scale, xlab='Station', ylab='Error', main='Histogram')
plot(density(all$err_scale),xlab='Station', ylab='Error', main='Density')
abline(v=quantile(all$err_scale, c(0.05,0.95)), lty=2, col ='red')

plot(all$err_scale, type='l', xlab='Station', ylab='Error scale parameter', main='Series')
abline(h=mean(all$err_scale), col='red', type='dashed')

adf.test(all$err_scale)
```

```{r}
shapiro.test(all$err_shape)
hist(all$err_shape, xlab='Station', ylab='Error', main='Histogrma')
plot(density(all$err_shape), xlab='Station', ylab='Error', main='Density')
abline(v=quantile(all$err_shape, c(0.05,0.95)), lty=2, col ='red')

plot(all$err_shape, type='l', xlab='Station', ylab='Error shape parameter', main='Series')
abline(h=mean(all$err_shape), col='red')

adf.test(all$err_shape)
```

Plot error scale parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_11 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = all,
    aes(Longitude, Latitude, color= err_scale), size = 1
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(-20,20),colours = rainbow(3), name = "Error scale parameter")+
  theme(plot.title = element_text(hjust = 0.5))
map_11
```

Plot error shape parameter.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_12 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = all,
    aes(Longitude, Latitude, color= err_shape), size = 1
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(-2,2),colours = rainbow(3), name = "Error shape parameter")+
  theme(plot.title = element_text(hjust = 0.5))
map_12
```

Plot error wet days.

```{r}
theme_update(plot.title = element_text(hjust = 0.5))
map_13 <- ggplot() +
  geom_map(
    data = world, map = world,
    aes(long, lat, map_id = region),
    color = "black", fill = "lightgray", size = 0.3
  ) +
  geom_point(
    data = all,
    aes(Longitude, Latitude, color= err_wet), size = 1
  ) +
 labs(title = 'World stations', x = "Longitude", y = "Latitude") +
    annotation_north_arrow(location = "bl", which_north = "true", 
        pad_x = unit(0.2, "in"), pad_y = unit(0.2, "in"),
        style = north_arrow_fancy_orienteering)+
  scale_color_gradientn(limits = c(-80,80),colours = rainbow(3), name = "Error wet days")+
  theme(plot.title = element_text(hjust = 0.5))
map_13
```

## New variables 

Before performing extrapolation we decide to add other variables. These are variables related to climate conditions available from WorldClim platform (https://worldclim.org/).

```{r}
r <- getData("worldclim",var="bio",res=10) #if problems restart R
r <- r[[c(1,3,7,12)]]
points <- all %>% dplyr::select(8,7)

values <- extract(r,points)
all_2 <- cbind.data.frame(all,values)
all_2$Count.y <- as.numeric(all_2$Count.y)
all_2$err_wet <- as.numeric(all_2$err_wet)
```

## Model wet days

The first variable under analysis is the number of wet days in 2018. We performs different types of models: linear regression, GAM, Artificial Neural Networks (ANN) and XGBoost.

```{r}
all_3 <- all_2 %>% dplyr::select(4,7,8,9,10,14,15,16,17)
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(1)

#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]

cat("RMSE:", rmse(test$Count.y, test$Count.x))
```

Linear regression.

```{r}
reg_l <- lm(Count.y~Count.x+Longitude+Latitude+bio1+bio7+bio12+Altitude, data=train)
summary(reg_l)

pred_reg <- predict(reg_l,test)

cat("RMSE:", rmse(test$Count.y, pred_reg))
```

GAM.

```{r}
GAM <- gam(Count.y~Count.x+ s(Longitude,bs="bs")+s(Latitude,bs="bs")+s(bio3, bs='bs')+s(bio7, bs='bs')+s(bio12,bs='bs')+s(Altitude,bs='bs'), data = train ,method="REML")
summary(GAM)

pred <- predict.gam(GAM,test)

cat("RMSE:", rmse(test$Count.y, pred))
```

XGBoost.

```{r}
#tuning parameyters
xtrain = data.matrix(train[,-5])
ytrain = train[,5]
xtest = data.matrix(test[,-5])
ytest = test[,5]

xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)

watchlist = list(train=xgb_train, test=xgb_test)

#create hyperparameter grid
max.depths = c(3, 6, 1)
etas = c(0.01, 0.02, 0.35)

best_params = 0
best_score = 0

count = 1
for( depth in max.depths ){
    for( num in etas){

        bst_grid = xgb.train(data = xgb_train, 
                                max.depth = depth, 
                                eta=num, 
                                nthread = 2, 
                                nround = 1000, 
                                watchlist = watchlist, 
                                objective = "reg:squarederror", 
                                early_stopping_rounds = 50, 
                                verbose=0)

        if(count == 1){
            best_params = bst_grid$params
            best_score = bst_grid$best_score
            count = count + 1
            }
        else if( bst_grid$best_score < best_score){
            best_params = bst_grid$params
            best_score = bst_grid$best_score
        }
    }
}

best_params
best_score
```

```{r}
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)

model_xgb = xgb.train(data = xgb_train,nthread = 2, eta = 0.02, max.depth = 3, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)

summary(model_xgb)

pred_xgb = predict(model_xgb, xgb_test)

cat("RMSE:", rmse(ytest, pred_xgb))

x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
       col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
```

```{r}
importance <- xgb.importance(feature_names = colnames(xgb_test),model = model_xgb)
print(xgb.ggplot.importance(importance_matrix = importance))
```

ANN.

```{r}

set.seed(1)

maxs <- apply(all_3, 2, max) 
mins <- apply(all_3, 2, min)
scaled <- as.data.frame(scale(all_3, center = mins, 
                              scale = maxs - mins))

train <- scaled[sample, ]
test <- scaled[!sample, ]

xtrain = as.matrix(train[,-5])
ytrain = as.matrix(train[,5])
xtest = as.matrix(test[,-5])
ytest = as.matrix(test[, 5])

xtrain = array(xtrain, dim = c(nrow(xtrain), dim(xtrain)[2], 1))
xtest = array(xtest, dim = c(nrow(xtest), dim(xtest)[2], 1))
dim(xtrain)
dim(xtest)

in_dim = c(dim(xtrain)[2:3])
print(in_dim)

model = keras_model_sequential() %>%
  layer_conv_1d(filters = 64, kernel_size = 2,
               input_shape = in_dim, activation = "relu") %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "linear")
 
model %>% compile(
  loss = "mse",
  optimizer = "adam")

model %>% summary()
```

```{r}
set.seed(1)
model %>% fit(xtrain, ytrain, epochs = 100, batch_size=16, verbose = 0)
scores = model %>% evaluate(xtrain, ytrain, verbose = 0)
print(scores)
```

```{r}
ypred = model %>% predict(xtest)

ypred <- ypred * (max(all_3$Count.y) - min(all_3$Count.y)) + min(all_3$Count.y)
ytest <- (ytest) * (max(all_3$Count.y) - min(all_3$Count.y)) +min(all_3$Count.y)

cat("RMSE:", rmse(ytest, ypred))
```

```{r}
x_axes = seq(1:length(ypred))
plot(x_axes, ytest, ylim = c(min(ypred), max(ytest)),
     col = "blue", type = "l", lwd = 2, ylab = "Wet days", xlab = 'Stations')
lines(x_axes, ypred, col = "red", type = "l", lwd = 2)
legend("topleft", legend = c("y-test", "y-pred"),
       col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
```

The XGBoost turned out to be the best model in terms of RMSE.

## Model scale

The same procedure is performed considering the scale parameter.

```{r}
all_3 <- all_2 %>% dplyr::select(3,4,6,7,8,9,10,14,15,16,17)
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(1)

#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]

cat("RMSE:", rmse(test$Scale.y, test$Scale.x))
```

Linear regression.

```{r}
#Linear regression
reg_l <- lm(Scale.y~Scale.x+Longitude+Latitude+bio7+bio1+bio12, data=train)
summary(reg_l)

pred_reg <- predict(reg_l,test)

cat("RMSE:", rmse(test$Scale.y, pred_reg))
```

GAM.

```{r}
GAM <- gam(Scale.y ~ s(Scale.x, bs="cc")+s(Longitude,bs="bs")+s(Latitude,bs="bs")+s(bio3,bs="bs")+s(bio7,bs="bs"), data = train ,method="REML")
summary(GAM)

pred <- predict.gam(GAM,test)

cat("RMSE:", rmse(test$Scale.y, pred))
```

xGBoost

```{r}
#tunning parameters
xtrain = data.matrix(train[,-3])
ytrain = train[,3]
xtest = data.matrix(test[,-3])
ytest = test[,3]

xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)

watchlist = list(train=xgb_train, test=xgb_test)

#create hyperparameter grid
max.depths = c(1, 2, 3, 4, 5, 6)
etas = c(0.01, 0.02, 0.1, 0.35, 0.4,0.5)

best_params = 0
best_score = 0

count = 1
for( depth in max.depths ){
    for( num in etas){

        bst_grid = xgb.train(data = xgb_train, 
                                max.depth = depth, 
                                eta=num, 
                                nthread = 2, 
                                nround = 1000, 
                                watchlist = watchlist, 
                                objective = "reg:squarederror", 
                                early_stopping_rounds = 50, 
                                verbose=0)

        if(count == 1){
            best_params = bst_grid$params
            best_score = bst_grid$best_score
            count = count + 1
            }
        else if( bst_grid$best_score < best_score){
            best_params = bst_grid$params
            best_score = bst_grid$best_score
        }
    }
}

best_params
best_score
```

```{r}
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)

model_xgb = xgb.train(data = xgb_train,nthread = 2, eta = 0.35, max.depth = 3, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)

summary(model_xgb)

pred_xgb = predict(model_xgb, xgb_test)

cat("RMSE:", rmse(ytest, pred_xgb))

x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
       col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
```

```{r}
importance <- xgb.importance(feature_names = colnames(xgb_test),model = model_xgb)
print(xgb.ggplot.importance(importance_matrix = importance))
```

ANN.

```{r}
set.seed(1)

maxs <- apply(all_3, 2, max) 
mins <- apply(all_3, 2, min)
scaled <- as.data.frame(scale(all_3, center = mins, 
                              scale = maxs - mins))

train <- scaled[sample, ]
test <- scaled[!sample, ]

xtrain = as.matrix(train[,-3])
ytrain = as.matrix(train[,3])
xtest = as.matrix(test[,-3])
ytest = as.matrix(test[, 3])

xtrain = array(xtrain, dim = c(nrow(xtrain), dim(xtrain)[2], 1))
xtest = array(xtest, dim = c(nrow(xtest), dim(xtest)[2], 1))
dim(xtrain)
dim(xtest)

in_dim = c(dim(xtrain)[2:3])
print(in_dim)

model = keras_model_sequential() %>%
  layer_conv_1d(filters = 64, kernel_size = 2,
               input_shape = in_dim, activation = "relu") %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "linear")
 
model %>% compile(
  loss = "mse",
  optimizer = "adam")

model %>% summary()
```

```{r}
model %>% fit(xtrain, ytrain, epochs = 100, batch_size=16, verbose = 0)
scores = model %>% evaluate(xtrain, ytrain, verbose = 0)
print(scores)
```

```{r}
ypred = model %>% predict(xtest)

ypred = model %>% predict(xtest)

ypred <- ypred* (max(all_3$Scale.y) - min(all_3$Scale.y)) + min(all_3$Scale.y)
ytest <- (ytest) * (max(all_3$Scale.y) - min(all_3$Scale.y)) +min(all_3$Scale.y)

cat("RMSE:", rmse(ytest, ypred))
```

```{r}
x_axes = seq(1:length(ypred))
plot(x_axes, ytest, ylim = c(min(ypred), max(ytest)),
     col = "blue", type = "l", lwd = 2, ylab = "Wet days", xlab = 'Stations')
lines(x_axes, ypred, col = "red", type = "l", lwd = 2)
legend("topleft", legend = c("y-test", "y-pred"),
       col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
```

## Model shape

The same procedure is performed considering the shape parameter.

```{r}
all_3 <- all_2 %>% dplyr::select(2,4,5,7,8,9,14,15,16,17)
all_3[is.na(all_3)] <- 0
#make this example reproducible
set.seed(1)

#use 80% of dataset as training set and 20% as test set
sample <- sample(c(TRUE, FALSE), nrow(all_3), replace=TRUE, prob=c(0.8,0.2))
train <- all_3[sample, ]
test <- all_3[!sample, ]

cat("RMSE:", rmse(test$Shape.y, test$Shape.x))
```

Linear regression.

```{r}
reg_l <- lm(Shape.y~Longitude+Count.x, data=train)
summary(reg_l)

pred_reg <- predict(reg_l,test)

cat("RMSE:", rmse(test$Shape.y, pred_reg))
```

GAM.

```{r}
#GAM
GAM <- gam(Shape.y ~Longitude + s(Count.x,bs="bs"), data = train ,method="REML")
summary(GAM)

pred <- predict.gam(GAM,test)

cat("RMSE:", rmse(test$Shape.y, pred))
```

XGBoost.

```{r}
#tunning parameters
xtrain = data.matrix(train[,-3])
ytrain = train[,3]
xtest = data.matrix(test[,-3])
ytest = test[,3]

xgb_train = xgb.DMatrix(data = xtrain, label = ytrain)
xgb_test = xgb.DMatrix(data = xtest, label = ytest)

watchlist = list(train=xgb_train, test=xgb_test)

#create hyperparameter grid
max.depths = c(1, 2, 3, 4, 5, 6)
etas = c(0.01, 0.02, 0.1, 0.35, 0.4, 0.45, 0.5, 0.6)

best_params = 0
best_score = 0

count = 1
for( depth in max.depths ){
    for( num in etas){

        bst_grid = xgb.train(data = xgb_train, 
                                max.depth = depth, 
                                eta=num, 
                                nthread = 2, 
                                nround = 1000, 
                                watchlist = watchlist, 
                                objective = "reg:squarederror", 
                                early_stopping_rounds = 50, 
                                verbose=0)

        if(count == 1){
            best_params = bst_grid$params
            best_score = bst_grid$best_score
            count = count + 1
            }
        else if( bst_grid$best_score < best_score){
            best_params = bst_grid$params
            best_score = bst_grid$best_score
        }
    }
}

best_params
best_score
```

```{r}
#fit XGBoost model and display training and testing data at each iteartion
#xgb.train(data = xgb_train, max.depth = 2, watchlist=watchlist, nrounds = 1000)

model_xgb = xgb.train(data = xgb_train, nthread = 2, eta = 0.5, max.depth = 2, nrounds = 1000, watchlist = watchlist, objective = "reg:squarederror", early_stopping_rounds = 50,  verbose = 1)

summary(model_xgb)

pred_xgb = predict(model_xgb, xgb_test)

cat("RMSE:", rmse(ytest, pred_xgb))

x = 1:length(ytest)                   # visualize the model, actual and predicted data
plot(x, ytest, col = "blue", type = "l", xlab='Stations',ylab = 'Scale parameter')
lines(x, pred_xgb, col = "red", type = "l")
legend("topleft", legend = c("y-test", "y-pred"),
       col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
```

```{r}
importance <- xgb.importance(feature_names = colnames(xgb_test),model = model_xgb)
print(xgb.ggplot.importance(importance_matrix = importance))
```

ANN.

```{r}
xtrain = as.matrix(train[,-3])
ytrain = as.matrix(train[,3])
xtest = as.matrix(test[,-3])
ytest = as.matrix(test[, 3])

xtrain = array(xtrain, dim = c(nrow(xtrain), dim(xtrain)[2], 1))
xtest = array(xtest, dim = c(nrow(xtest), dim(xtest)[2], 1))
dim(xtrain)
dim(xtest)

in_dim = c(dim(xtrain)[2:3])
print(in_dim)

model = keras_model_sequential() %>%
  layer_conv_1d(filters = 64, kernel_size = 2,
               input_shape = in_dim, activation = "relu") %>%
  layer_flatten() %>%
  layer_dense(units = 32, activation = "relu") %>%
  layer_dense(units = 1, activation = "linear")
 
model %>% compile(
  loss = "mse",
  optimizer = "adam")

model %>% summary()
```

```{r}
model %>% fit(xtrain, ytrain, epochs = 100, batch_size=16, verbose = 0)
scores = model %>% evaluate(xtrain, ytrain, verbose = 0)
print(scores)
```

```{r}
ypred = model %>% predict(xtest)

ypred = model %>% predict(xtest)

ypred <- ypred* (max(all_3$Shape.y) - min(all_3$Shape.y)) + min(all_3$Shape.y)
ytest <- (ytest) * (max(all_3$Shape.y) - min(all_3$Shape.y)) +min(all_3$Shape.y)

cat("RMSE:", rmse(ytest, ypred))
```

```{r}
x_axes = seq(1:length(ypred))
plot(x_axes, ytest, ylim = c(min(ypred), max(ytest)),
     col = "blue", type = "l", lwd = 2, ylab = "Shape parameter", xlab = 'Stations')
lines(x_axes, ypred, col = "red", type = "l", lwd = 2)
legend("topleft", legend = c("y-test", "y-pred"),
       col = c("blue", "red"), lty=1, cex=0.7, lwd=2, bty='n')
```






